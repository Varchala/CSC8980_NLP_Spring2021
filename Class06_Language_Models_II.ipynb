{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Class06-Language Models II.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOzln0wV6Z4mBImbSBWw+cZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmbanda/CSC8980_NLP_Spring2021/blob/main/Class06_Language_Models_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JMGC7Y0sy-3"
      },
      "source": [
        "# Class 6 - Language Models II\r\n",
        "\r\n",
        "In this Google Colab notebook, we will go over a very basic example of a Language Model implementation using the Reuters corpus that comes with the NLTK NLP python library (we will cover NLTK in more detail at a later class session)\r\n",
        "\r\n",
        "The Reuters corpus is a collection of 10,788 news documents totaling 1.3 million words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxRc9PqIhTym"
      },
      "source": [
        "In order to use some of NLTK's built-in functionality, we need to import the libary (already installed in Colab) and download the Reuters corpus and Punkt to be able to process its data more easily. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkVzaFu8tIXq"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('reuters')\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm9uW1emhjVD"
      },
      "source": [
        "In the following piece of code we are importing both the bigrams and trigrams methods from NLTK and calculate the co-occurence frequencies and convert them in probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mKHzpeWsydf"
      },
      "source": [
        "from nltk.corpus import reuters\r\n",
        "from nltk import bigrams, trigrams\r\n",
        "from collections import Counter, defaultdict\r\n",
        "\r\n",
        "# Create a placeholder for model\r\n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))\r\n",
        "\r\n",
        "# Count frequency of co-occurence  \r\n",
        "for sentence in reuters.sents():\r\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\r\n",
        "        model[(w1, w2)][w3] += 1\r\n",
        " \r\n",
        "# Let's transform the counts to probabilities\r\n",
        "for w1_w2 in model:\r\n",
        "    total_count = float(sum(model[w1_w2].values()))\r\n",
        "    for w3 in model[w1_w2]:\r\n",
        "        model[w1_w2][w3] /= total_count"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI7_We47iaqb"
      },
      "source": [
        "Let's now evaluate what we have done: we will start with two simple words – “today the”. We want our model to tell us what will be the next word:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR3KZox8vhUP"
      },
      "source": [
        "dict(model[\"today\",\"the\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvwlO7KoobAs"
      },
      "source": [
        "Let's try something that could me more common now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XakhIlMvsGV"
      },
      "source": [
        "dict(model[\"the\",\"price\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwa3gtHbo3iT"
      },
      "source": [
        "Now that we have our model trained, let's try to generate text using this model. The idea here is to set a random probability threshold and continue to append words based on their probability of being next until the threshold is met."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JX7sam9v7Go"
      },
      "source": [
        "import random\r\n",
        "\r\n",
        "# starting words\r\n",
        "text = [\"today\", \"the\"]\r\n",
        "sentence_finished = False\r\n",
        " \r\n",
        "while not sentence_finished:\r\n",
        "  # select a random probability threshold  \r\n",
        "  r = random.random()\r\n",
        "  accumulator = .0\r\n",
        "\r\n",
        "  for word in model[tuple(text[-2:])].keys():\r\n",
        "      accumulator += model[tuple(text[-2:])][word]\r\n",
        "      # select words that are above the probability threshold\r\n",
        "      if accumulator >= r:\r\n",
        "          text.append(word)\r\n",
        "          break\r\n",
        "\r\n",
        "  if text[-2:] == [None, None]:\r\n",
        "      sentence_finished = True\r\n",
        " \r\n",
        "print (' '.join([t for t in text if t]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJI6BCqntByp"
      },
      "source": [
        "## Additional practice assignments:\r\n",
        "\r\n",
        "1) The code here uses trigrams, can you adat it to use bigrams? how about quadrigrams?\r\n",
        "\r\n",
        "2) Can you feed use the Shakespeare file from HW1 or his complete works to generate some text?\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Code Sources:\r\n",
        "\r\n",
        "https://nlpforhackers.io/language-models/\r\n",
        "\r\n"
      ]
    }
  ]
}