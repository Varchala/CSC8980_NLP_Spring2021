{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Class10-11 Part of Speech tagging.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmbanda/CSC8980_NLP_Spring2021/blob/main/Class10_11_Part_of_Speech_tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQWFO0PsoGOI"
      },
      "source": [
        "# Building a Part of Speech Tagger from 'scratch'\n",
        "\n",
        "Source: https://colab.research.google.com/drive/1d7LO_0665DYw6DrVJXXautJAJzHHqYOm\n",
        "\n",
        "In this notebook, I'll guide you through the steps of training some models to be further utilized in our NLP Tool to do PoS Tagging. Here we won't apply any state of the art algorithm, but we won't be far either!\n",
        "\n",
        "\n",
        "## Getting the data (Corpus)\n",
        "\n",
        "Let us start by where we'll get our data (our **corpus**). There are many sources, but two are the most commonly used:\n",
        "* **Penn Treebank** subset from nltk.\n",
        "* The **Universal Dependencies** Treebanks, available (as of February 2020) for 90 languages (in different quality and quantity levels). \n",
        "\n",
        "These contain the hard work of many **annotators**, which went through selected sets of sentences and annotated each one by hand, forming a corpus to be used as **supervised** input for our **machine learning algorithms**.\n",
        "\n",
        "The following two cells will show how to import the corpus from each of these two sources."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B5gdrh7Z_Ic"
      },
      "source": [
        "#This cell loads the Penn Treebank corpus from nltk into a list variable named penn_treebank.\n",
        "\n",
        "import nltk\n",
        "\n",
        "#Ensure that the treebank corpus is downloaded\n",
        "nltk.download('treebank')\n",
        "\n",
        "#Load the treebank corpus class\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "#Now we iterate over all samples from the corpus (the fileids - that are equivalent to sentences) \n",
        "#and retrieve the word and the pre-labeled PoS tag. This will be added as a list of tuples with \n",
        "#a list of words and a list of their respective PoS tags (in the same order).\n",
        "penn_treebank = []\n",
        "for fileid in treebank.fileids():\n",
        "  tokens = []\n",
        "  tags = []\n",
        "  for word, tag in treebank.tagged_words(fileid):\n",
        "    tokens.append(word)\n",
        "    tags.append(tag)\n",
        "  penn_treebank.append((tokens, tags))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WwZYkNr1bPN"
      },
      "source": [
        "#This cell loads the Universal Dependecies Treekbank corpus. It'll download all the packages, but we'll only use the GUM\n",
        "#english package. We'll also install the conllu package, that was developed to parse data in the conLLu format, a \n",
        "#format common of linguistic annotated files. We'll also have a list variable, but now named ud_treebank.\n",
        "\n",
        "#Install conllu package, download the UD Treebanks corpus and unpack it.\n",
        "!pip install conllu\n",
        "!wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-3105/ud-treebanks-v2.5.tgz\n",
        "!tar zxf ud-treebanks-v2.5.tgz\n",
        "\n",
        "#The imports needed to open and parse (interpret) the conllu file. At the end we'll have a list of dicts.\n",
        "from io import open\n",
        "from conllu import parse_incr\n",
        "\n",
        "#Open the file and load the sentences to a list.\n",
        "data_file = open(\"ud-treebanks-v2.5/UD_English-GUM/en_gum-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
        "ud_files = []\n",
        "for tokenlist in parse_incr(data_file):\n",
        "    ud_files.append(tokenlist)\n",
        "\n",
        "#Now we iterate over all samples from the corpus and retrieve the word and the pre-labeled PoS tag (upostag). This will \n",
        "#be added as a list of tuples with a list of words and a list of their respective PoS tags (in the same order).\n",
        "ud_treebank = []\n",
        "for sentence in ud_files:\n",
        "  tokens = []\n",
        "  tags = []\n",
        "  for token in sentence:\n",
        "    tokens.append(token['form'])\n",
        "    tags.append(token['upostag'])\n",
        "  ud_treebank.append((tokens, tags))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzzGnG10Ulv2"
      },
      "source": [
        "**Word of Caution!**\n",
        "\n",
        "Penn Treebank and UD Treebanks use *distinct tagsets*. \n",
        "\n",
        "We won't be able to interchange them unless we make a converter - also, we'll only be able to do so from Penn->UD, because Penn Treebank has tags more detailed than UD.\n",
        "\n",
        "Let us continue with the explanation of the Tagger."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfD5ujGijuUF"
      },
      "source": [
        "## Extracting Features form Words\n",
        "\n",
        "Next, we have to create a function that is able to extract features from our words. These features will be used to predict the PoS.\n",
        "\n",
        "For that,  for each word, we'll pass the sentence and word index, and we'll provide a dict with the features.\n",
        "\n",
        "To explain about the feature set (can be changed, if you want), it is composed by:\n",
        "* Word: the word itself. Some words are always one PoS, others not.\n",
        "* is_first, is_last: check if it is the first or last in the sentence.\n",
        "* is_capitalized: first letter is caps? Maybe it is a proper noun...\n",
        "* is_all_caps or is_all_lower: checks for acronyms (or common words).\n",
        "* prefixes/suffixes: check word initialization/termination\n",
        "* prev_word/next_word: checks the preceding and succeding word.\n",
        "* has-hyphen: words with '-' may be adjectives.\n",
        "* is_numeric: for numbers.\n",
        "* capitals_inside: weird cases. Maybe nouns.\n",
        "\n",
        "The basis of this feature extraction method comes from two nice articles:\n",
        "* https://nlpforhackers.io/training-pos-tagger/\n",
        "* https://medium.com/analytics-vidhya/pos-tagging-using-conditional-random-fields-92077e5eaa31\n",
        "\n",
        "If you're wondering, yes, this encoding WILL need a lot of memory for training (if you're not using categorical variables).\n",
        "\n",
        "And we'll have to replicate this in our main code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IyIaTSwoo-V"
      },
      "source": [
        "#Regex module for checking alphanumeric values.\n",
        "import re\n",
        "def extract_features(sentence, index):\n",
        "  return {\n",
        "      'word':sentence[index],\n",
        "      'is_first':index==0,\n",
        "      'is_last':index ==len(sentence)-1,\n",
        "      'is_capitalized':sentence[index][0].upper() == sentence[index][0],\n",
        "      'is_all_caps': sentence[index].upper() == sentence[index],\n",
        "      'is_all_lower': sentence[index].lower() == sentence[index],\n",
        "      'is_alphanumeric': int(bool((re.match('^(?=.*[0-9]$)(?=.*[a-zA-Z])',sentence[index])))),\n",
        "      'prefix-1':sentence[index][0],\n",
        "      'prefix-2':sentence[index][:2],\n",
        "      'prefix-3':sentence[index][:3],\n",
        "      'prefix-3':sentence[index][:4],\n",
        "      'suffix-1':sentence[index][-1],\n",
        "      'suffix-2':sentence[index][-2:],\n",
        "      'suffix-3':sentence[index][-3:],\n",
        "      'suffix-3':sentence[index][-4:],\n",
        "      'prev_word':'' if index == 0 else sentence[index-1],\n",
        "      'next_word':'' if index < len(sentence) else sentence[index+1],\n",
        "      'has_hyphen': '-' in sentence[index],\n",
        "      'is_numeric': sentence[index].isdigit(),\n",
        "      'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYS5r1_m6Yr9"
      },
      "source": [
        "We now prepare the dataset for use in Machine Learning algorithms.\n",
        "\n",
        "There are two steps to it: \n",
        "* Defining a function to transform the corpus to a more datsetish format.\n",
        "* Then, divide the encoded data into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hiniE_wzPOC"
      },
      "source": [
        "#Ater defining the extract_features, we define a simple function to transform our data in a more 'datasetish' format.\n",
        "#This function returns the data as two lists, one of Dicts of features and the other with the labels.\n",
        "def transform_to_dataset(tagged_sentences):\n",
        "  X, y = [], []\n",
        "  for sentence, tags in tagged_sentences:\n",
        "    sent_word_features, sent_tags = [],[]\n",
        "    for index in range(len(sentence)):\n",
        "        sent_word_features.append(extract_features(sentence, index)),\n",
        "        sent_tags.append(tags[index])\n",
        "    X.append(sent_word_features)\n",
        "    y.append(sent_tags)\n",
        "  return X, y\n",
        "\n",
        "#We divide the set BEFORE encoding. Why? To have full sentences in training/testing sets. When we encode, we do not encode\n",
        "#a sentence, but its words instead.\n",
        "\n",
        "#First, for the Penn treebank.\n",
        "penn_train_size = int(0.8*len(penn_treebank))\n",
        "penn_training = penn_treebank[:penn_train_size]\n",
        "penn_testing = penn_treebank[penn_train_size:]\n",
        "X_penn_train, y_penn_train = transform_to_dataset(penn_training)\n",
        "X_penn_test, y_penn_test = transform_to_dataset(penn_testing)\n",
        "\n",
        "#Then, for UD Treebank.\n",
        "ud_train_size = int(0.8*len(ud_treebank))\n",
        "ud_training = ud_treebank[:ud_train_size]\n",
        "ud_testing = ud_treebank[ud_train_size:]\n",
        "X_ud_train, y_ud_train = transform_to_dataset(ud_training)\n",
        "X_ud_test, y_ud_test = transform_to_dataset(ud_testing)\n",
        "\n",
        "#Third step, vectorize datasets. For that we use sklearn DictVectorizer\n",
        "#WARNING"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFc51jNtDptW"
      },
      "source": [
        "## Training a Tagger\n",
        "\n",
        "Now, we can train supervised machine learning algorithms to PoS Tagging.\n",
        "\n",
        "We'll use the Conditional Random Fields (CRF) algorithm. Here's a brief explanation:\n",
        "\n",
        "* **CRF**: A variation of Markov Random Field. Okay, that might not have helped. It is a discriminative model that, in a quick summary, evaluates the probabilities that a set of states are dependant or not between themselves based on a set of observations. In this case, it evaluates the probabilities that a word observed in a context (defined by the above mentioned features) belongs to a specific PoS. In training time, it takes what is the best state given the set of current observations and probabilities.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://miro.medium.com/max/681/1*8hOWH7YF5INMF2OPhKjVxA.png\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "Want more math? Read this: https://towardsdatascience.com/conditional-random-fields-explained-e5b8256da776\n",
        "\n",
        "So, to achieve this, we'll use scikit learn (sklearn) and a sklearn compatible crf suite (skleran_crfsuit)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHTkotyWpd28"
      },
      "source": [
        "#Ignoring some warnings for the sake of readability.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#First, install sklearn_crfsuite, as it is not preloaded into Colab. \n",
        "!pip install sklearn_crfsuite\n",
        "from sklearn_crfsuite import CRF\n",
        "\n",
        "#This loads the model. Specifics are: \n",
        "#algorithm: methodology used to check if results are improving. Default is lbfgs (gradient descent).\n",
        "#c1 and c2:  coefficients used for regularization.\n",
        "#max_iterations: max number of iterations\n",
        "#all_possible_transitions: since crf creates a \"network\", of probability transition states,\n",
        "#this option allows it to map even \"connections\" not present in the data.\n",
        "penn_crf = CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.01,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "#The fit method is the default name used by Machine Learning algorithms to start training.\n",
        "print(\"Started training on Penn Treebank corpus!\")\n",
        "penn_crf.fit(X_penn_train, y_penn_train)\n",
        "print(\"Finished training on Penn Treebank corpus!\")\n",
        "\n",
        "#Same for UD\n",
        "ud_crf = CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.01,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "print(\"Started training on UD corpus!\")\n",
        "ud_crf.fit(X_ud_train, y_ud_train)\n",
        "print(\"Finished training on UD corpus!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQvig1nQBcbA"
      },
      "source": [
        "## Checking the Results\n",
        "\n",
        "For that, we'll use a score method named balanced f-score. This score takes into account *precision* and *recall*.\n",
        "\n",
        "* **precision**: Considering the universe of tagged words, how many were correctly tagged?\n",
        "* **recall**: Considering the universe of correct tags, how many words were really correctly tagged?\n",
        "\n",
        "The distinction is in the direction you look. Precision looks at all tagged words to find how many are ok; Recall looks at correct tags to find how many were able to be \"guessed\".\n",
        "\n",
        "F-score is then calculated using these two. I won't go into the maths of it.  If you want,\n",
        "* You can read the wikipedia article here: https://en.wikipedia.org/wiki/F1_score\n",
        "* Or watch a neat simple video here: https://www.youtube.com/watch?v=j-EB6RqqjGI&ab_channel=CodeEmporium\n",
        "\n",
        "Also, here's the wikipedia image to help you understand:\n",
        "<div>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png\"/>\n",
        "</div>\n",
        "\n",
        "We won't go into the computations either. Let the package do its thing (after all, we're interested in NLP now, not in statistics):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTlJwNkF_0zs"
      },
      "source": [
        "#We'll use the sklearn_crfsuit own metrics to compute f1 score.\n",
        "from sklearn_crfsuite import metrics\n",
        "from sklearn_crfsuite import scorers\n",
        "print(\"## Penn ##\")\n",
        "\n",
        "#First calculate a prediction from test data, then we print the metrics for f-1 using the .flat_f1_score method.\n",
        "y_penn_pred=penn_crf.predict(X_penn_test)\n",
        "print(\"F1 score on Test Data\")\n",
        "print(metrics.flat_f1_score(y_penn_test, y_penn_pred,average='weighted',labels=penn_crf.classes_))\n",
        "#For the sake of clarification, we do the same for train data.\n",
        "y_penn_pred_train=penn_crf.predict(X_penn_train)\n",
        "print(\"F1 score on Training Data \")\n",
        "print(metrics.flat_f1_score(y_penn_train, y_penn_pred_train,average='weighted',labels=penn_crf.classes_))\n",
        "\n",
        "# This presents class wise score. Helps see which classes (tags) are the ones with most problems.\n",
        "print(\"Class wise score:\")\n",
        "print(metrics.flat_classification_report(\n",
        "    y_penn_test, y_penn_pred, labels=penn_crf.classes_, digits=3\n",
        "))\n",
        "\n",
        "#Same for UD\n",
        "print(\"## UD ##\")\n",
        "\n",
        "y_ud_pred=ud_crf.predict(X_ud_test)\n",
        "print(\"F1 score on Test Data \")\n",
        "print(metrics.flat_f1_score(y_ud_test, y_ud_pred,average='weighted',labels=ud_crf.classes_))\n",
        "y_ud_pred_train=ud_crf.predict(X_ud_train)\n",
        "print(\"F1 score on Training Data \")\n",
        "print(metrics.flat_f1_score(y_ud_train, y_ud_pred_train,average='weighted',labels=ud_crf.classes_))\n",
        "\n",
        "### Look at class wise score\n",
        "print(\"Class wise score:\")\n",
        "print(metrics.flat_classification_report(\n",
        "    y_ud_test, y_ud_pred, labels=ud_crf.classes_, digits=3\n",
        "))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJUcgYacWTaa"
      },
      "source": [
        "Not too shabby!\n",
        "\n",
        "Remember that State of the Art results for Penn Treebank are at 97% f1.\n",
        "\n",
        "Now, notice how UD is worse (90%)? Probably because there aren't many tags, so less variation and less classes for probability distribution.\n",
        "\n",
        "---\n",
        "\n",
        "But, wouldn't it be better if we could see it actually working?\n",
        "\n",
        "That's what the following cell does. It also helps us understand what we'll have to implement in our main algorithm for it to work.\n",
        "\n",
        "Feel free to play with the input phrase.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4rcQq-ubbuT"
      },
      "source": [
        "#First, we pass the sentence and \"quickly tokenize it\" - we've already done it in our code, so I'll just mock here with a split:\n",
        "sent = \"The tagger produced good results\"\n",
        "features = [extract_features(sent.split(), idx) for idx in range(len(sent.split()))]\n",
        "\n",
        "#Then we tell the algorithm to make a prediction on a single input (sentence). I'll do once for Penn Treebank and once for UD.\n",
        "penn_results = penn_crf.predict_single(features)\n",
        "ud_results = ud_crf.predict_single(features)\n",
        "\n",
        "#These line magics are just there to make it a neaty print, making a (word, POS) style print;\n",
        "penn_tups = [(sent.split()[idx], penn_results[idx]) for idx in range(len(sent.split()))]\n",
        "ud_tups = [(sent.split()[idx], ud_results[idx]) for idx in range(len(sent.split()))]\n",
        "\n",
        "#The results come out here! Notice the difference in tags.\n",
        "print(penn_tups)\n",
        "print(ud_tups)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mn831Zxbckt"
      },
      "source": [
        "## Saving the Weights\n",
        "\n",
        "We will want to load this to our NLPTools, right? So we have to save the weights. This means saving the classifier we trained to be able to classify our tokens.\n",
        "\n",
        "To do it, we use Pickle, which is a Python package to save a readable binary file extension called \"pickle\". We'll later open this in our tool.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul2KlQu4c5-N"
      },
      "source": [
        "#import the pickle module\n",
        "import pickle\n",
        "\n",
        "#Simply dump! Use 'wb' in open to write bytes.\n",
        "\n",
        "penn_filename = 'penn_treebank_crf_postagger.sav'\n",
        "pickle.dump(penn_crf, open(penn_filename, 'wb'))\n",
        "\n",
        "ud_filename = 'ud_crf_postagger.sav'\n",
        "pickle.dump(ud_crf, open(ud_filename,'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O2F5HijfWc8"
      },
      "source": [
        "To open the file, we just have to import the module and read the file using:\n",
        "\n",
        "`model = pickle.load(open(filename, 'rb'))`\n",
        "\n",
        "Great, we now have pickle files that can be loaded in our tool. Just download them using the lefthand file explorer and we're good to go!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsZUdhjO7x9b"
      },
      "source": [
        "# Using Spacy for Part of Speech Tagging\r\n",
        "\r\n",
        "The _spaCy_ framework — along with a wide and growing range of plug-ins and other integrations — provides features for a wide range of natural language tasks.\r\n",
        "It's become one of the most widely used natural language libraries in Python for industry use cases, and has quite a large community — and with that, much support for commercialization of research advances as this area continues to evolve rapidly.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w__08YmB71Le"
      },
      "source": [
        "COde code code "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I54ThY30755N"
      },
      "source": [
        "import spacy\r\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATUZuEKo784t"
      },
      "source": [
        "text = \"The rain in Spain falls mainly on the plain.\"\r\n",
        "doc = nlp(text)\r\n",
        "\r\n",
        "for token in doc:\r\n",
        "    print(token.text, token.lemma_, token.pos_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4HGkfej8IlF"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "cols = (\"text\", \"lemma\", \"POS\", \"explain\", \"stopword\")\r\n",
        "rows = []\r\n",
        "\r\n",
        "for t in doc:\r\n",
        "    row = [t.text, t.lemma_, t.pos_, spacy.explain(t.pos_), t.is_stop]\r\n",
        "    rows.append(row)\r\n",
        "\r\n",
        "df = pd.DataFrame(rows, columns=cols)\r\n",
        "    \r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVw21qk88Rnx"
      },
      "source": [
        "from spacy import displacy\r\n",
        "\r\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUH3UvZF8Vuo"
      },
      "source": [
        "text = \"We were all out at the zoo one day, I was doing some acting, walking on the railing of the gorilla exhibit. I fell in. Everyone screamed and Tommy jumped in after me, forgetting that he had blueberries in his front pocket. The gorillas just went wild.\"\r\n",
        "\r\n",
        "doc = nlp(text)\r\n",
        "\r\n",
        "for sent in doc.sents:\r\n",
        "    print(\">\", sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i0D8Xg3RGaW"
      },
      "source": [
        "## Using WordNet in Spacy\r\n",
        "\r\n",
        "There's a _spaCy_ integration for WordNet called\r\n",
        "[spacy-wordnet](https://github.com/recognai/spacy-wordnet) by [Daniel Vila Suero](https://twitter.com/dvilasuero), an expert in natural language and knowledge graph work.\r\n",
        "\r\n",
        "Then we'll load the WordNet data via NLTK (these things happen):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEVa0aqeRI0C"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('wordnet')\r\n",
        "!pip install spacy-wordnet\r\n",
        "\r\n",
        "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\r\n",
        "\r\n",
        "print(\"before\", nlp.pipe_names)\r\n",
        "\r\n",
        "if \"WordnetAnnotator\" not in nlp.pipe_names:\r\n",
        "    nlp.add_pipe(WordnetAnnotator(nlp.lang), after=\"tagger\")\r\n",
        "    \r\n",
        "print(\"after\", nlp.pipe_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNrT4-M4Reif"
      },
      "source": [
        "Note that _spaCy_ runs as a \"pipeline\" and allows means for customizing parts of the pipeline in use.\r\n",
        "That's excellent for supporting really interesting workflow integrations in data science work.\r\n",
        "Here we used the `WordnetAnnotator` from the _spacy-wordnet_ project\r\n",
        "\r\n",
        "Within the English language, some words are infamous for having many possible meanings. For example, click through the results online in a [WordNet](http://wordnetweb.princeton.edu/perl/webwn?s=star&sub=Search+WordNet&o2=&o0=1&o8=1&o1=1&o7=&o5=&o9=&o6=&o3=&o4=&h=) search to find the meanings related to the word `withdraw`.\r\n",
        "\r\n",
        "Now let's use _spaCy_ to perform that lookup automatically:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eglNkQNbRetT"
      },
      "source": [
        "token = nlp(\"withdraw\")[0]\r\n",
        "token._.wordnet.synsets()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jur-w1u0R0tz"
      },
      "source": [
        "token._.wordnet.lemmas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXB_jiF7R2EO"
      },
      "source": [
        "token._.wordnet.wordnet_domains()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb2Qob9AP5V0"
      },
      "source": [
        "Again, if you're working with knowledge graphs, those \"word sense\" links from WordNet could be used along with graph algorithms to help identify the meanings for a particular word. That can also be used to develop summaries for larger sections of text through a technique called _summarization_.  It's beyond the scope of this tutorial, but an interesting application currently for natural language in industry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi4SXxuyP5V0"
      },
      "source": [
        "Going in the other direction, if you know _a priori_ that a document was about a particular domain or set of topics, then you can constrain the meanings returned from _WordNet_. In the following example, we want to consider NLU results that are within Finance and Banking:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsGO8ouCR9zm"
      },
      "source": [
        "domains = [\"finance\", \"banking\"]\r\n",
        "sentence = nlp(\"I want to withdraw 5,000 euros.\")\r\n",
        "\r\n",
        "enriched_sent = []\r\n",
        "\r\n",
        "for token in sentence:\r\n",
        "    # get synsets within the desired domains\r\n",
        "    synsets = token._.wordnet.wordnet_synsets_for_domain(domains)\r\n",
        "    \r\n",
        "    if synsets:\r\n",
        "        lemmas_for_synset = []\r\n",
        "        \r\n",
        "        for s in synsets:\r\n",
        "            # get synset variants and add to the enriched sentence\r\n",
        "            lemmas_for_synset.extend(s.lemma_names())\r\n",
        "            enriched_sent.append(\"({})\".format(\"|\".join(set(lemmas_for_synset))))\r\n",
        "    else:\r\n",
        "        enriched_sent.append(token.text)\r\n",
        "\r\n",
        "print(\" \".join(enriched_sent))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN2TyobQ9QE-"
      },
      "source": [
        "# Using NLTK for Part of Speech Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odZFgigp9Vz6"
      },
      "source": [
        "Python’s NLTK library features a robust sentence tokenizer and POS tagger. Python has a native tokenizer, the .split() function, which you can pass a separator and it will split the string that the function is called on on that separator. The NLTK tokenizer is more robust. It tokenizes a sentence into words and punctuation. Given the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrIdlep49WoS"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "\r\n",
        "tokens = nltk.word_tokenize(\"Can you please buy me an Arizona Ice Tea? It's $0.99.\")\r\n",
        "\r\n",
        "print(\"Tokens: \", tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8MR-stV908W"
      },
      "source": [
        "print(\"Part of speech tagging: \", nltk.pos_tag(tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHelk4HDX-6l"
      },
      "source": [
        "As you can see on the code above, the .pos_tag() function needs to be passed a tokenized sentence for tagging. The tagging is done by way of a trained model in the NLTK library. The included POS tagger is not perfect but it does yield pretty accurate results. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1__F0NMsXcIl"
      },
      "source": [
        "# Sources:\r\n",
        "\r\n",
        "The code in this notebook as been used and adapted from:\r\n",
        "\r\n",
        "Part of Speech tagger:\r\n",
        "https://colab.research.google.com/drive/1d7LO_0665DYw6DrVJXXautJAJzHHqYOm\r\n",
        "\r\n",
        "Spacy Tutorials:\r\n",
        "https://colab.research.google.com/github/DerwenAI/spaCy_tuTorial/blob/master/spaCy_tuTorial.ipynb\r\n",
        "\r\n",
        "NLTK:\r\n",
        "https://medium.com/@gianpaul.r/tokenization-and-parts-of-speech-pos-tagging-in-pythons-nltk-library-2d30f70af13b\r\n",
        "\r\n",
        "All credit goes to the original authors."
      ]
    }
  ]
}