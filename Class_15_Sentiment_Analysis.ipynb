{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Class 15 - Sentiment Analysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmbanda/CSC8980_NLP_Spring2021/blob/main/Class_15_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZhf6HenVnl2"
      },
      "source": [
        "# Designing your own sentiment analysis tool\n",
        "\n",
        "While there are a lot of tools that will automatically give us a sentiment of a piece of text, we learned that they don't always agree! Let's design our own to see both how these tools work internally, along with how we can test them to see how well they might perform.\n",
        "\n",
        "**Original Source: https://colab.research.google.com/github/littlecolumns/ds4j-notebooks/blob/master/investigating-sentiment-analysis/notebooks/Designing%20your%20own%20sentiment%20analysis%20tool.ipynb.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWf72bp2VnmF"
      },
      "source": [
        "### Prep work: Downloading necessary files\n",
        "Before we get started, we need to download all of the data we'll be using.\n",
        "* **sentiment140-subset.csv:** cleaned subset of Sentiment140 data - half a million tweets marked as positive or negative\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d31PgWX1VnmG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39c4dd6a-9785-40a7-c0b7-4422bafc46d5"
      },
      "source": [
        "# Make data directory if it doesn't exist\n",
        "!mkdir -p data\n",
        "!wget -nc https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/investigating-sentiment-analysis/data/sentiment140-subset.csv.zip -P data\n",
        "!unzip -n -d data data/sentiment140-subset.csv.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-02 23:49:31--  https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/investigating-sentiment-analysis/data/sentiment140-subset.csv.zip\n",
            "Resolving nyc3.digitaloceanspaces.com (nyc3.digitaloceanspaces.com)... 162.243.189.2\n",
            "Connecting to nyc3.digitaloceanspaces.com (nyc3.digitaloceanspaces.com)|162.243.189.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17927149 (17M) [application/zip]\n",
            "Saving to: ‘data/sentiment140-subset.csv.zip’\n",
            "\n",
            "sentiment140-subset 100%[===================>]  17.10M  23.2MB/s    in 0.7s    \n",
            "\n",
            "2021-03-02 23:49:32 (23.2 MB/s) - ‘data/sentiment140-subset.csv.zip’ saved [17927149/17927149]\n",
            "\n",
            "Archive:  data/sentiment140-subset.csv.zip\n",
            "  inflating: data/sentiment140-subset.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVfUj987VnmI"
      },
      "source": [
        "## Training on tweets\n",
        "\n",
        "Let's say we were going to analyze the sentiment of tweets. If we had a list of tweets that were scored positive vs. negative, we could see which words are usually associated with positive scores and which are usually associated with negative scores.\n",
        "\n",
        "Luckily, we have **Sentiment140** - http://help.sentiment140.com/for-students - a list of 1.6 million tweets along with a score as to whether they're negative or positive. We'll use it to build our own machine learning algorithm to see separate positivity from negativity.\n",
        "\n",
        "### Read in our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9izUi5NyVnmI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2412c02b-8c76-49b1-b226-efacd30af305"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"data/sentiment140-subset.csv\", nrows=30000)\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@kconsidder You never tweet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Sick today  coding from the couch.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>@ChargerJenn Thx for answering so quick,I was ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Wii fit says I've lost 10 pounds since last ti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@MrKinetik Not a thing!!!  I don't really have...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   polarity                                               text\n",
              "0         0                      @kconsidder You never tweet  \n",
              "1         0                 Sick today  coding from the couch.\n",
              "2         1  @ChargerJenn Thx for answering so quick,I was ...\n",
              "3         1  Wii fit says I've lost 10 pounds since last ti...\n",
              "4         0  @MrKinetik Not a thing!!!  I don't really have..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR9c243-VnmL"
      },
      "source": [
        "It isn't a very complicated dataset. `polarity` is whether it's positive or not, `text` is the text of the tweet itself.\n",
        "\n",
        "How many rows do we have?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRhD357zVnmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63fff751-ef2b-440e-d9d5-d9dbb261c934"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04I8-Y7oVnmM"
      },
      "source": [
        "How many **positive** tweets compared to how many **negative** tweets?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnaHJdMMVnmN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f1837b-e8aa-4dc4-9fc0-eff5952acec9"
      },
      "source": [
        "df.polarity.value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    15064\n",
              "0    14936\n",
              "Name: polarity, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDkgoY0YVnmN"
      },
      "source": [
        "## Train our algorithm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ids80XVrVnmO"
      },
      "source": [
        "### Vectorize our tweets\n",
        "\n",
        "Create a `TfidfVectorizer` and use it to vectorize our tweets. Since we don't have all the time in the world, we should probably use `max_features` to only take a selection of terms - how about 1000 for now?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7etZdraVnmO"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0wIKKWsVnmP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "907879ce-1608-4792-b343-31ebd878442c"
      },
      "source": [
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "vectors = vectorizer.fit_transform(df.text)\n",
        "words_df = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names())\n",
        "words_df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>10</th>\n",
              "      <th>100</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>15</th>\n",
              "      <th>1st</th>\n",
              "      <th>20</th>\n",
              "      <th>2day</th>\n",
              "      <th>2nd</th>\n",
              "      <th>30</th>\n",
              "      <th>able</th>\n",
              "      <th>about</th>\n",
              "      <th>account</th>\n",
              "      <th>actually</th>\n",
              "      <th>add</th>\n",
              "      <th>after</th>\n",
              "      <th>afternoon</th>\n",
              "      <th>again</th>\n",
              "      <th>ago</th>\n",
              "      <th>agree</th>\n",
              "      <th>ah</th>\n",
              "      <th>ahh</th>\n",
              "      <th>ahhh</th>\n",
              "      <th>air</th>\n",
              "      <th>album</th>\n",
              "      <th>all</th>\n",
              "      <th>almost</th>\n",
              "      <th>alone</th>\n",
              "      <th>already</th>\n",
              "      <th>alright</th>\n",
              "      <th>also</th>\n",
              "      <th>although</th>\n",
              "      <th>always</th>\n",
              "      <th>am</th>\n",
              "      <th>amazing</th>\n",
              "      <th>amp</th>\n",
              "      <th>an</th>\n",
              "      <th>and</th>\n",
              "      <th>annoying</th>\n",
              "      <th>another</th>\n",
              "      <th>...</th>\n",
              "      <th>work</th>\n",
              "      <th>worked</th>\n",
              "      <th>working</th>\n",
              "      <th>works</th>\n",
              "      <th>world</th>\n",
              "      <th>worried</th>\n",
              "      <th>worry</th>\n",
              "      <th>worse</th>\n",
              "      <th>worst</th>\n",
              "      <th>worth</th>\n",
              "      <th>would</th>\n",
              "      <th>wouldn</th>\n",
              "      <th>wow</th>\n",
              "      <th>write</th>\n",
              "      <th>writing</th>\n",
              "      <th>wrong</th>\n",
              "      <th>wtf</th>\n",
              "      <th>www</th>\n",
              "      <th>xd</th>\n",
              "      <th>xoxo</th>\n",
              "      <th>xx</th>\n",
              "      <th>xxx</th>\n",
              "      <th>ya</th>\n",
              "      <th>yay</th>\n",
              "      <th>yea</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>years</th>\n",
              "      <th>yep</th>\n",
              "      <th>yes</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>yet</th>\n",
              "      <th>yo</th>\n",
              "      <th>you</th>\n",
              "      <th>young</th>\n",
              "      <th>your</th>\n",
              "      <th>yourself</th>\n",
              "      <th>youtube</th>\n",
              "      <th>yum</th>\n",
              "      <th>yup</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.334095</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.22101</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.427465</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         10  100   11   12   15  1st  ...  young  your  yourself  youtube  yum  yup\n",
              "0  0.000000  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0       0.0      0.0  0.0  0.0\n",
              "1  0.000000  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0       0.0      0.0  0.0  0.0\n",
              "2  0.000000  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0       0.0      0.0  0.0  0.0\n",
              "3  0.427465  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0       0.0      0.0  0.0  0.0\n",
              "4  0.000000  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0       0.0      0.0  0.0  0.0\n",
              "\n",
              "[5 rows x 1000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3iy178_VnmP"
      },
      "source": [
        "### Setting up our variables\n",
        "\n",
        "Because we want to fit in with all the other progammers, we need to create two variables: one called `X` and one called `y`.\n",
        "\n",
        "`X` is all of our **features**, the things we use to predict positive or negative. That's going to be our words.\n",
        "\n",
        "`y` is all of our **labels**, the positive or negative rating. We'll use the `polarity` column for that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7uTJHpfVnmQ"
      },
      "source": [
        "X = words_df\n",
        "y = df.polarity"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ena3fNiVnmQ"
      },
      "source": [
        "### Picking an algorithm\n",
        "\n",
        "What kind of algorithm do we want? Who knows, we don't know anything about machine learning! **Let's just pick ALL OF THEM.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAMDEIYJVnmR"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99EXbq-KVnmR"
      },
      "source": [
        "### Training our algorithms\n",
        "\n",
        "When we teach our algorithm about what a positive or a negative tweet looks like, this is called **training**. Training can take different amounts of time based on what kind of algorithm you are using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXxhqQeAVnmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4ad6fe3-ec7f-4ef9-e76b-b740313a201e"
      },
      "source": [
        "%%time\n",
        "# Create and train a logistic regression\n",
        "logreg = LogisticRegression(C=1e9, solver='lbfgs', max_iter=1000)\n",
        "logreg.fit(X, y)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 14 s, sys: 821 ms, total: 14.8 s\n",
            "Wall time: 7.52 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJNUq1LEVnmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb5406a2-ff39-4da4-b5b1-512f0bdade76"
      },
      "source": [
        "%%time\n",
        "# Create and train a random forest classifier\n",
        "forest = RandomForestClassifier(n_estimators=50)\n",
        "forest.fit(X, y)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 29.9 s, sys: 87.5 ms, total: 30 s\n",
            "Wall time: 30 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Agcl6fFlVnmT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5453f6c-1ab0-45f9-8fc1-406a3d9d1bc5"
      },
      "source": [
        "%%time\n",
        "# Create and train a linear support vector classifier (LinearSVC)\n",
        "svc = LinearSVC()\n",
        "svc.fit(X, y)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 395 ms, sys: 0 ns, total: 395 ms\n",
            "Wall time: 400 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of1QHdXSVnmT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac46fd60-4369-4964-9f12-f86a278240cb"
      },
      "source": [
        "%%time\n",
        "# Create and train a multinomial naive bayes classifier (MultinomialNB)\n",
        "bayes = MultinomialNB()\n",
        "bayes.fit(X, y)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 184 ms, sys: 4.92 ms, total: 189 ms\n",
            "Wall time: 127 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KUG1V-PVnmU"
      },
      "source": [
        "Think about: **How long did each take to train?** How much faster were some compared to others?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK-YgO6HVnmU"
      },
      "source": [
        "## Use our models\n",
        "\n",
        "Now that we've trained our models, **they can try to predict whether some content is positive or negative**.\n",
        "\n",
        "### Preparing the data\n",
        "\n",
        "**Add a few more sentences below.** They should be a mix of positive and negative. They can be boring, they can be exciting, they can be short, they can be long."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH6JmpWJVnmV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "ef7586c6-9bbc-4ad7-d7f2-905de0828e5c"
      },
      "source": [
        "# Create some test data\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "\n",
        "unknown = pd.DataFrame({'content': [\n",
        "    \"I love love love love this kitten\",\n",
        "    \"I hate hate hate hate this keyboard\",\n",
        "    \"I'm not sure how I feel about toast\",\n",
        "    \"Did you see the baseball game yesterday?\",\n",
        "    \"The package was delivered late and the contents were broken\",\n",
        "    \"Trashy television shows are some of my favorites\",\n",
        "    \"I'm seeing a Kubrick film tomorrow, I hear not so great things about it.\",\n",
        "    \"I find chirping birds irritating, but I know I'm not the only one\",\n",
        "]})\n",
        "unknown"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I love love love love this kitten</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I hate hate hate hate this keyboard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I'm not sure how I feel about toast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Did you see the baseball game yesterday?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The package was delivered late and the contents were broken</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Trashy television shows are some of my favorites</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I'm seeing a Kubrick film tomorrow, I hear not so great things about it.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I find chirping birds irritating, but I know I'm not the only one</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                    content\n",
              "0                                         I love love love love this kitten\n",
              "1                                       I hate hate hate hate this keyboard\n",
              "2                                       I'm not sure how I feel about toast\n",
              "3                                  Did you see the baseball game yesterday?\n",
              "4               The package was delivered late and the contents were broken\n",
              "5                          Trashy television shows are some of my favorites\n",
              "6  I'm seeing a Kubrick film tomorrow, I hear not so great things about it.\n",
              "7         I find chirping birds irritating, but I know I'm not the only one"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAX3ZVzIVnmV"
      },
      "source": [
        "First we need to **vectorizer** our sentences into numbers, so the algorithm can understand them.\n",
        "\n",
        "Our algorithm only knows **certain words.** Run `vectorizer.get_feature_names()` to show you the list of the words it knows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVRTNo9bVnmW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c77d330-e2ce-41eb-842d-3642936d9f2a"
      },
      "source": [
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['10', '100', '11', '12', '15', '1st', '20', '2day', '2nd', '30', 'able', 'about', 'account', 'actually', 'add', 'after', 'afternoon', 'again', 'ago', 'agree', 'ah', 'ahh', 'ahhh', 'air', 'album', 'all', 'almost', 'alone', 'already', 'alright', 'also', 'although', 'always', 'am', 'amazing', 'amp', 'an', 'and', 'annoying', 'another', 'any', 'anymore', 'anyone', 'anything', 'anyway', 'app', 'apparently', 'apple', 'appreciate', 'are', 'around', 'art', 'as', 'ask', 'asleep', 'ass', 'at', 'ate', 'aw', 'awake', 'awards', 'away', 'awesome', 'aww', 'awww', 'baby', 'back', 'bad', 'band', 'bbq', 'bday', 'be', 'beach', 'beautiful', 'because', 'bed', 'been', 'beer', 'before', 'behind', 'being', 'believe', 'best', 'bet', 'better', 'big', 'bike', 'birthday', 'bit', 'bitch', 'black', 'blip', 'blog', 'blue', 'body', 'boo', 'book', 'books', 'bored', 'boring', 'both', 'bought', 'bout', 'box', 'boy', 'boys', 'break', 'breakfast', 'bring', 'bro', 'broke', 'broken', 'brother', 'brothers', 'btw', 'bus', 'business', 'busy', 'but', 'buy', 'by', 'bye', 'cake', 'call', 'called', 'came', 'can', 'cannot', 'cant', 'car', 'card', 'care', 'case', 'cat', 'catch', 'cause', 'cd', 'chance', 'change', 'channel', 'chat', 'check', 'chicken', 'chocolate', 'church', 'city', 'class', 'clean', 'cleaning', 'close', 'closed', 'club', 'coffee', 'cold', 'college', 'com', 'come', 'comes', 'coming', 'completely', 'computer', 'concert', 'congrats', 'cool', 'cos', 'could', 'couldn', 'country', 'couple', 'course', 'crap', 'crazy', 'cream', 'cry', 'crying', 'cut', 'cute', 'cuz', 'da', 'dad', 'damn', 'dance', 'date', 'daughter', 'david', 'day', 'days', 'ddlovato', 'de', 'dead', 'dear', 'decided', 'definitely', 'did', 'didn', 'didnt', 'die', 'died', 'dinner', 'dm', 'do', 'does', 'doesn', 'doesnt', 'dog', 'doing', 'don', 'done', 'dont', 'down', 'download', 'dream', 'dreams', 'dress', 'drink', 'drinking', 'drive', 'driving', 'drunk', 'dude', 'due', 'during', 'each', 'earlier', 'early', 'easy', 'eat', 'eating', 'either', 'else', 'em', 'email', 'end', 'ended', 'english', 'enjoy', 'enjoyed', 'enjoying', 'enough', 'episode', 'especially', 'even', 'evening', 'ever', 'every', 'everybody', 'everyone', 'everything', 'exactly', 'exam', 'exams', 'except', 'excited', 'exciting', 'eye', 'eyes', 'face', 'facebook', 'fact', 'fail', 'fair', 'fall', 'family', 'fan', 'fans', 'far', 'fast', 'favorite', 'fb', 'feel', 'feeling', 'feels', 'feet', 'fell', 'felt', 'few', 'ff', 'figure', 'final', 'finally', 'finals', 'find', 'fine', 'fingers', 'finish', 'finished', 'fire', 'first', 'fix', 'flight', 'flu', 'fly', 'fm', 'follow', 'followers', 'followfriday', 'following', 'food', 'for', 'forever', 'forget', 'forgot', 'forward', 'found', 'free', 'friday', 'friend', 'friends', 'from', 'front', 'fuck', 'fucking', 'full', 'fun', 'funny', 'game', 'games', 'garden', 'gave', 'gd', 'get', 'gets', 'getting', 'girl', 'girls', 'give', 'glad', 'go', 'god', 'goes', 'goin', 'going', 'gone', 'gonna', 'good', 'goodbye', 'goodnight', 'google', 'got', 'gotta', 'graduation', 'great', 'green', 'gt', 'guess', 'guitar', 'guy', 'guys', 'gym', 'ha', 'had', 'haha', 'hahaha', 'hair', 'half', 'hand', 'hang', 'happen', 'happened', 'happens', 'happy', 'hard', 'has', 'hate', 'hates', 'have', 'haven', 'havent', 'having', 'he', 'head', 'headache', 'headed', 'heading', 'hear', 'heard', 'heart', 'hehe', 'hell', 'hello', 'help', 'her', 'here', 'hey', 'hi', 'high', 'him', 'his', 'history', 'hit', 'hmm', 'holiday', 'home', 'homework', 'hope', 'hopefully', 'hoping', 'horrible', 'hot', 'hotel', 'hour', 'hours', 'house', 'how', 'http', 'hubby', 'hug', 'huge', 'hugs', 'hun', 'hungry', 'hurt', 'hurts', 'ice', 'idea', 'idk', 'if', 'ill', 'im', 'in', 'inside', 'instead', 'interesting', 'internet', 'into', 'iphone', 'ipod', 'is', 'isn', 'isnt', 'it', 'its', 'ive', 'jealous', 'job', 'join', 'jonas', 'jonasbrothers', 'july', 'june', 'jus', 'just', 'keep', 'keeps', 'kid', 'kids', 'kill', 'kind', 'kinda', 'knew', 'know', 'knows', 'la', 'lady', 'lakers', 'lame', 'laptop', 'last', 'late', 'later', 'laugh', 'lazy', 'learn', 'learning', 'least', 'leave', 'leaving', 'left', 'less', 'let', 'lets', 'life', 'like', 'liked', 'lil', 'line', 'link', 'list', 'listen', 'listening', 'little', 'live', 'living', 'll', 'lmao', 'lol', 'london', 'lonely', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'lost', 'lot', 'lots', 'love', 'loved', 'lovely', 'loves', 'loving', 'lt', 'luck', 'lucky', 'lunch', 'luv', 'ly', 'ma', 'mac', 'mad', 'made', 'mail', 'major', 'make', 'makes', 'making', 'man', 'many', 'maths', 'matter', 'may', 'maybe', 'mcfly', 'me', 'mean', 'means', 'meant', 'meet', 'meeting', 'message', 'met', 'might', 'miley', 'mileycyrus', 'mind', 'mine', 'minute', 'minutes', 'miss', 'missed', 'missing', 'mom', 'moment', 'monday', 'money', 'month', 'months', 'mood', 'moon', 'more', 'morning', 'most', 'mother', 'mouth', 'move', 'movie', 'movies', 'moving', 'mr', 'mtv', 'much', 'mum', 'music', 'must', 'my', 'myloc', 'myself', 'myspace', 'name', 'nap', 'near', 'need', 'needed', 'needs', 'never', 'new', 'news', 'next', 'nice', 'night', 'nights', 'nite', 'no', 'nope', 'not', 'nothing', 'now', 'number', 'of', 'off', 'office', 'oh', 'ohh', 'ok', 'okay', 'old', 'omg', 'on', 'once', 'one', 'ones', 'online', 'only', 'open', 'or', 'other', 'ouch', 'our', 'out', 'outside', 'over', 'own', 'packing', 'page', 'pain', 'paper', 'parents', 'park', 'part', 'party', 'pass', 'past', 'pay', 'peace', 'people', 'perfect', 'person', 'phone', 'photo', 'photos', 'pic', 'pick', 'pics', 'picture', 'pictures', 'pink', 'pizza', 'place', 'plan', 'plans', 'play', 'played', 'playing', 'please', 'pls', 'plurk', 'plus', 'point', 'pool', 'poor', 'post', 'posted', 'power', 'ppl', 'pretty', 'probably', 'problem', 'profile', 'project', 'proud', 'put', 'quite', 'quot', 'radio', 'rain', 'raining', 'rainy', 'random', 'rather', 're', 'read', 'reading', 'ready', 'real', 'realized', 'really', 'reason', 'red', 'relaxing', 'remember', 'reply', 'rest', 'revision', 'ride', 'right', 'rip', 'road', 'rock', 'room', 'run', 'running', 'sad', 'sadly', 'safe', 'said', 'same', 'sat', 'saturday', 'save', 'saw', 'say', 'saying', 'says', 'scared', 'scary', 'school', 'season', 'second', 'see', 'seeing', 'seem', 'seems', 'seen', 'send', 'sent', 'seriously', 'set', 'shall', 'shame', 'share', 'she', 'shirt', 'shit', 'shoes', 'shop', 'shopping', 'short', 'should', 'show', 'shower', 'shows', 'sick', 'side', 'sigh', 'sign', 'silly', 'sims', 'since', 'singing', 'sister', 'site', 'sitting', 'sleep', 'sleeping', 'sleepy', 'slept', 'slow', 'small', 'smile', 'so', 'some', 'someone', 'something', 'sometimes', 'son', 'song', 'songs', 'soo', 'soon', 'sooo', 'soooo', 'sore', 'sorry', 'sound', 'sounds', 'special', 'spend', 'spending', 'spent', 'star', 'start', 'started', 'starting', 'starts', 'stay', 'still', 'stomach', 'stop', 'store', 'story', 'straight', 'stuck', 'study', 'studying', 'stuff', 'stupid', 'such', 'suck', 'sucks', 'summer', 'sun', 'sunday', 'sunny', 'sunshine', 'super', 'support', 'supposed', 'sure', 'sweet', 'take', 'takes', 'taking', 'talk', 'talking', 'taylor', 'tea', 'team', 'tell', 'test', 'text', 'than', 'thank', 'thanks', 'that', 'thats', 'the', 'their', 'them', 'then', 'there', 'these', 'they', 'thing', 'things', 'think', 'thinking', 'thinks', 'this', 'tho', 'those', 'though', 'thought', 'three', 'throat', 'through', 'thru', 'thursday', 'thx', 'tickets', 'til', 'till', 'time', 'times', 'tinyurl', 'tired', 'to', 'today', 'together', 'told', 'tom', 'tommcfly', 'tomorrow', 'tonight', 'too', 'took', 'top', 'totally', 'tour', 'town', 'traffic', 'train', 'tried', 'trip', 'true', 'try', 'trying', 'tuesday', 'turn', 'tv', 'tweet', 'tweeting', 'tweets', 'twilight', 'twitpic', 'twitter', 'two', 'ugh', 'uk', 'under', 'understand', 'unfortunately', 'until', 'up', 'update', 'updates', 'upset', 'ur', 'us', 'use', 'used', 'using', 'vacation', 've', 'vegas', 'version', 'very', 'via', 'video', 'visit', 'voice', 'wait', 'waiting', 'wake', 'walk', 'wanna', 'want', 'wanted', 'wants', 'warm', 'was', 'wasn', 'watch', 'watched', 'watching', 'water', 'way', 'we', 'wear', 'weather', 'website', 'wedding', 'wednesday', 'week', 'weekend', 'weeks', 'weird', 'welcome', 'well', 'went', 'were', 'what', 'whats', 'when', 'where', 'which', 'while', 'white', 'who', 'whole', 'why', 'wife', 'will', 'win', 'wine', 'wish', 'wishes', 'wishing', 'wit', 'with', 'without', 'woke', 'won', 'wonder', 'wonderful', 'wondering', 'wont', 'woo', 'word', 'words', 'work', 'worked', 'working', 'works', 'world', 'worried', 'worry', 'worse', 'worst', 'worth', 'would', 'wouldn', 'wow', 'write', 'writing', 'wrong', 'wtf', 'www', 'xd', 'xoxo', 'xx', 'xxx', 'ya', 'yay', 'yea', 'yeah', 'year', 'years', 'yep', 'yes', 'yesterday', 'yet', 'yo', 'you', 'young', 'your', 'yourself', 'youtube', 'yum', 'yup']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kegoq0MfVnmW"
      },
      "source": [
        "Usually when we use the vectorizer, we write code like this:\n",
        "    \n",
        "```python\n",
        "vectors = vectorizer.fit_transform(....)\n",
        "```\n",
        "\n",
        "Which both learns all the words **and** counts them. In this case **we already have the list of words we know, we only want to count them.** So instead of `.fit_transform`, we just use `.transform`:\n",
        "\n",
        "```python\n",
        "unknown_vectors = vectorizer.transform(unknown.content)\n",
        "unknown_words_df = ......\n",
        "```\n",
        "\n",
        "Finish making your `unknown_words_df` in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQYd9VrqVnmX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "11fc2d92-ab8c-44ba-9b9f-ada32484e309"
      },
      "source": [
        "# Put it through the vectoriser\n",
        "\n",
        "# transform, not fit_transform, because we already learned all our words\n",
        "unknown_vectors = vectorizer.transform(unknown.content)\n",
        "unknown_words_df = pd.DataFrame(unknown_vectors.toarray(), columns=vectorizer.get_feature_names())\n",
        "unknown_words_df.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>10</th>\n",
              "      <th>100</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>15</th>\n",
              "      <th>1st</th>\n",
              "      <th>20</th>\n",
              "      <th>2day</th>\n",
              "      <th>2nd</th>\n",
              "      <th>30</th>\n",
              "      <th>able</th>\n",
              "      <th>about</th>\n",
              "      <th>account</th>\n",
              "      <th>actually</th>\n",
              "      <th>add</th>\n",
              "      <th>after</th>\n",
              "      <th>afternoon</th>\n",
              "      <th>again</th>\n",
              "      <th>ago</th>\n",
              "      <th>agree</th>\n",
              "      <th>ah</th>\n",
              "      <th>ahh</th>\n",
              "      <th>ahhh</th>\n",
              "      <th>air</th>\n",
              "      <th>album</th>\n",
              "      <th>all</th>\n",
              "      <th>almost</th>\n",
              "      <th>alone</th>\n",
              "      <th>already</th>\n",
              "      <th>alright</th>\n",
              "      <th>also</th>\n",
              "      <th>although</th>\n",
              "      <th>always</th>\n",
              "      <th>am</th>\n",
              "      <th>amazing</th>\n",
              "      <th>amp</th>\n",
              "      <th>an</th>\n",
              "      <th>and</th>\n",
              "      <th>annoying</th>\n",
              "      <th>another</th>\n",
              "      <th>...</th>\n",
              "      <th>work</th>\n",
              "      <th>worked</th>\n",
              "      <th>working</th>\n",
              "      <th>works</th>\n",
              "      <th>world</th>\n",
              "      <th>worried</th>\n",
              "      <th>worry</th>\n",
              "      <th>worse</th>\n",
              "      <th>worst</th>\n",
              "      <th>worth</th>\n",
              "      <th>would</th>\n",
              "      <th>wouldn</th>\n",
              "      <th>wow</th>\n",
              "      <th>write</th>\n",
              "      <th>writing</th>\n",
              "      <th>wrong</th>\n",
              "      <th>wtf</th>\n",
              "      <th>www</th>\n",
              "      <th>xd</th>\n",
              "      <th>xoxo</th>\n",
              "      <th>xx</th>\n",
              "      <th>xxx</th>\n",
              "      <th>ya</th>\n",
              "      <th>yay</th>\n",
              "      <th>yea</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>years</th>\n",
              "      <th>yep</th>\n",
              "      <th>yes</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>yet</th>\n",
              "      <th>yo</th>\n",
              "      <th>you</th>\n",
              "      <th>young</th>\n",
              "      <th>your</th>\n",
              "      <th>yourself</th>\n",
              "      <th>youtube</th>\n",
              "      <th>yum</th>\n",
              "      <th>yup</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.417209</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.537291</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.244939</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.215967</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    10  100   11   12   15  1st  ...  young  your  yourself  youtube  yum  yup\n",
              "0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0       0.0      0.0  0.0  0.0\n",
              "1  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0       0.0      0.0  0.0  0.0\n",
              "2  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0       0.0      0.0  0.0  0.0\n",
              "3  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0       0.0      0.0  0.0  0.0\n",
              "4  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0       0.0      0.0  0.0  0.0\n",
              "\n",
              "[5 rows x 1000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnU9MwFiVnmX"
      },
      "source": [
        "Confirm `unknown_words_df` is 11 rows and 2,000 columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aUFslz1VnmY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38337cb0-ccf2-4676-f870-c05f50fbcdb1"
      },
      "source": [
        "unknown_words_df.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NST3ar3VnmY"
      },
      "source": [
        "### Predicting with our models\n",
        "\n",
        "To make a prediction for each of our sentences, you can use `.predict` with each of our models. For example, it would look like this for linear regression:\n",
        "\n",
        "```python\n",
        "unknown['pred_logreg'] = logreg.predict(unknown_words_df)\n",
        "```\n",
        "\n",
        "To add the prediction for logistic regression, you'd run similar `.predict` code, which will give you a `0` (negative) or a `1` (positive). A difference between the two is that for logistic regression, you can **also ask for the probability that the sentence is in the `1` category** instead of just simply the category. To do that, you use this code:\n",
        "\n",
        "```python\n",
        "unknown['pred_logreg_prob'] = linreg.predict_proba(unknown_words_df)[:,1]\n",
        "```\n",
        "\n",
        "**Add new columns for each of the models you trained.** If the model has a `.predict_proba`, add that as a column as well. \n",
        "\n",
        "* **Tip:** Tab is helpful for knowing whether `.predict_proba` is an option.\n",
        "* **Tip:** Don't forget the `[:,1]` after `.predict_proba`, it means \"give me the probability for category `1`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Desn33_aVnmZ"
      },
      "source": [
        "# Predict using all our models. \n",
        "\n",
        "# Logistic Regression predictions + probabilities\n",
        "unknown['pred_logreg'] = logreg.predict(unknown_words_df)\n",
        "unknown['pred_logreg_proba'] = logreg.predict_proba(unknown_words_df)[:,1]\n",
        "\n",
        "# Random forest predictions + probabilities\n",
        "unknown['pred_forest'] = forest.predict(unknown_words_df)\n",
        "unknown['pred_forest_proba'] = forest.predict_proba(unknown_words_df)[:,1]\n",
        "\n",
        "# SVC predictions\n",
        "unknown['pred_svc'] = svc.predict(unknown_words_df)\n",
        "\n",
        "# Bayes predictions + probabilities\n",
        "unknown['pred_bayes'] = bayes.predict(unknown_words_df)\n",
        "unknown['pred_bayes_proba'] = bayes.predict_proba(unknown_words_df)[:,1]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTDZ93kxVnmb",
        "outputId": "e5dbc7b7-dc87-47f4-c1bd-2b8be3cba7b3"
      },
      "source": [
        "unknown"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>pred_logreg</th>\n",
              "      <th>pred_logreg_proba</th>\n",
              "      <th>pred_forest</th>\n",
              "      <th>pred_forest_proba</th>\n",
              "      <th>pred_svc</th>\n",
              "      <th>pred_bayes</th>\n",
              "      <th>pred_bayes_proba</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I love love love love this kitten</td>\n",
              "      <td>1</td>\n",
              "      <td>0.950442</td>\n",
              "      <td>1</td>\n",
              "      <td>0.848665</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.747222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I hate hate hate hate this keyboard</td>\n",
              "      <td>0</td>\n",
              "      <td>0.009593</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.122383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I'm not sure how I feel about toast</td>\n",
              "      <td>0</td>\n",
              "      <td>0.180952</td>\n",
              "      <td>0</td>\n",
              "      <td>0.240000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.416819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Did you see the baseball game yesterday?</td>\n",
              "      <td>1</td>\n",
              "      <td>0.615063</td>\n",
              "      <td>1</td>\n",
              "      <td>0.660000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.509662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The package was delivered late and the contents were broken</td>\n",
              "      <td>0</td>\n",
              "      <td>0.058171</td>\n",
              "      <td>0</td>\n",
              "      <td>0.460000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.219788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Trashy television shows are some of my favorites</td>\n",
              "      <td>0</td>\n",
              "      <td>0.330293</td>\n",
              "      <td>0</td>\n",
              "      <td>0.440000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.534234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I'm seeing a Kubrick film tomorrow, I hear not so great things about it.</td>\n",
              "      <td>1</td>\n",
              "      <td>0.558548</td>\n",
              "      <td>0</td>\n",
              "      <td>0.260000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.533493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I find chirping birds irritating, but I know I'm not the only one</td>\n",
              "      <td>0</td>\n",
              "      <td>0.060122</td>\n",
              "      <td>0</td>\n",
              "      <td>0.440000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.295739</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                    content  \\\n",
              "0                                         I love love love love this kitten   \n",
              "1                                       I hate hate hate hate this keyboard   \n",
              "2                                       I'm not sure how I feel about toast   \n",
              "3                                  Did you see the baseball game yesterday?   \n",
              "4               The package was delivered late and the contents were broken   \n",
              "5                          Trashy television shows are some of my favorites   \n",
              "6  I'm seeing a Kubrick film tomorrow, I hear not so great things about it.   \n",
              "7         I find chirping birds irritating, but I know I'm not the only one   \n",
              "\n",
              "   pred_logreg  pred_logreg_proba  pred_forest  pred_forest_proba  pred_svc  \\\n",
              "0            1           0.950442            1           0.848665         1   \n",
              "1            0           0.009593            0           0.000000         0   \n",
              "2            0           0.180952            0           0.240000         0   \n",
              "3            1           0.615063            1           0.660000         1   \n",
              "4            0           0.058171            0           0.460000         0   \n",
              "5            0           0.330293            0           0.440000         0   \n",
              "6            1           0.558548            0           0.260000         1   \n",
              "7            0           0.060122            0           0.440000         0   \n",
              "\n",
              "   pred_bayes  pred_bayes_proba  \n",
              "0           1          0.747222  \n",
              "1           0          0.122383  \n",
              "2           0          0.416819  \n",
              "3           1          0.509662  \n",
              "4           0          0.219788  \n",
              "5           1          0.534234  \n",
              "6           1          0.533493  \n",
              "7           0          0.295739  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvg5YZAJVnmb"
      },
      "source": [
        "### Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzT4jZmgVnmb"
      },
      "source": [
        "* What do the numbers mean? What's the difference between a 0 and a 1? A 0.5? Negative numbers?\n",
        "* Were there any sentences where the classifiers seemed to disagree about? How do you feel about the amount they disagree? \n",
        "* What's the difference between using a 0/1 to talk about sentiment compared to 0-1? When might you use one compared to another?\n",
        "* What's the difference between the linear regression model and the other models we're using? Why might it fit or not fit?\n",
        "* Between 0-1, what range do you think counts as \"negative,\" \"positive\" and \"neutral\"?\n",
        "* Does the variation in scores reflect the variation you would see among people? Or is it better or worse?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX1tfrXfVnmc"
      },
      "source": [
        "## Testing our models\n",
        "\n",
        "We can actually see **which model performs the best!** Remember how we trained our models on tweets? We can ask each model about each tweet, and see if it gets the right answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZBvqPoWVnmd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "01a0f6fb-dd67-417d-eab4-67faf0c9fa02"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@kconsidder You never tweet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Sick today  coding from the couch.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>@ChargerJenn Thx for answering so quick,I was afraid I was gonna crash twitter with all the spamming I did 2 RR..sorry bout that</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Wii fit says I've lost 10 pounds since last time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@MrKinetik Not a thing!!!  I don't really have a life.....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   polarity                                                                                                                               text\n",
              "0         0                                                                                                      @kconsidder You never tweet  \n",
              "1         0                                                                                                 Sick today  coding from the couch.\n",
              "2         1  @ChargerJenn Thx for answering so quick,I was afraid I was gonna crash twitter with all the spamming I did 2 RR..sorry bout that \n",
              "3         1                                                                                Wii fit says I've lost 10 pounds since last time   \n",
              "4         0                                                                         @MrKinetik Not a thing!!!  I don't really have a life....."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ugGE6ncVnmd"
      },
      "source": [
        "Our original dataframe is a list of many, many tweets. We turned this into `X` - vectorized words - and `y` - whether the tweet is negative or positive.\n",
        "\n",
        "Before we used `.fit(X, y)` to train on all of our data. Instead, **we can test our models** by doing a test/train split and see if the predictions match the actual labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhYXF3HuVnme"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vGKovZtVnme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd3e41fc-cf27-4da6-d2fe-50dd8b8918a4"
      },
      "source": [
        "%%time\n",
        "\n",
        "print(\"Training logistic regression\")\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training random forest\")\n",
        "forest.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training SVC\")\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training Naive Bayes\")\n",
        "bayes.fit(X_train, y_train)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training logistic regression\n",
            "Training random forest\n",
            "Training SVC\n",
            "Training Naive Bayes\n",
            "CPU times: user 31.7 s, sys: 790 ms, total: 32.5 s\n",
            "Wall time: 26.8 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6NN7IaMVnmf"
      },
      "source": [
        "### Confusion matrices\n",
        "\n",
        "To see how well they did, we'll use a [\"confusion matrix\"](https://en.wikipedia.org/wiki/Confusion_matrix) for each one. I think confusion matrices are called that because they are confusing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYQ1kYQ5Vnmf"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89UKpKzDVnmf"
      },
      "source": [
        "#### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvWQE_MnVnmg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "c360bc04-34c8-4bee-f000-71bb3ff1aa12"
      },
      "source": [
        "y_true = y_test\n",
        "y_pred = logreg.predict(X_test)\n",
        "matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "label_names = pd.Series(['negative', 'positive'])\n",
        "pd.DataFrame(matrix,\n",
        "     columns='Predicted ' + label_names,\n",
        "     index='Is ' + label_names)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted negative</th>\n",
              "      <th>Predicted positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Is negative</th>\n",
              "      <td>2722</td>\n",
              "      <td>974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is positive</th>\n",
              "      <td>886</td>\n",
              "      <td>2918</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Predicted negative  Predicted positive\n",
              "Is negative                2722                 974\n",
              "Is positive                 886                2918"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJJqk1orVnmi"
      },
      "source": [
        "#### Random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMepCBheVnmi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "a43efbc1-0780-4ffb-83db-188b686820e8"
      },
      "source": [
        "y_true = y_test\n",
        "y_pred = forest.predict(X_test)\n",
        "matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "label_names = pd.Series(['negative', 'positive'])\n",
        "pd.DataFrame(matrix,\n",
        "     columns='Predicted ' + label_names,\n",
        "     index='Is ' + label_names)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted negative</th>\n",
              "      <th>Predicted positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Is negative</th>\n",
              "      <td>2764</td>\n",
              "      <td>932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is positive</th>\n",
              "      <td>1040</td>\n",
              "      <td>2764</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Predicted negative  Predicted positive\n",
              "Is negative                2764                 932\n",
              "Is positive                1040                2764"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHZhukuxVnmj"
      },
      "source": [
        "#### SVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR5xB6lhVnmj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "c3751d91-dc1e-484e-ee56-30d4c53e0c18"
      },
      "source": [
        "y_true = y_test\n",
        "y_pred = svc.predict(X_test)\n",
        "matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "label_names = pd.Series(['negative', 'positive'])\n",
        "pd.DataFrame(matrix,\n",
        "     columns='Predicted ' + label_names,\n",
        "     index='Is ' + label_names)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted negative</th>\n",
              "      <th>Predicted positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Is negative</th>\n",
              "      <td>2719</td>\n",
              "      <td>977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is positive</th>\n",
              "      <td>882</td>\n",
              "      <td>2922</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Predicted negative  Predicted positive\n",
              "Is negative                2719                 977\n",
              "Is positive                 882                2922"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eUfWv29Vnmj"
      },
      "source": [
        "#### Multinomial Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_Pba0lNVnmk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "cdf86158-6f2a-4212-a233-0d1831179c0c"
      },
      "source": [
        "y_true = y_test\n",
        "y_pred = bayes.predict(X_test)\n",
        "matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "label_names = pd.Series(['negative', 'positive'])\n",
        "pd.DataFrame(matrix,\n",
        "     columns='Predicted ' + label_names,\n",
        "     index='Is ' + label_names)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted negative</th>\n",
              "      <th>Predicted positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Is negative</th>\n",
              "      <td>2772</td>\n",
              "      <td>924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is positive</th>\n",
              "      <td>983</td>\n",
              "      <td>2821</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Predicted negative  Predicted positive\n",
              "Is negative                2772                 924\n",
              "Is positive                 983                2821"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV1IdchQVnmk"
      },
      "source": [
        "### Percentage-based confusion matrices\n",
        "\n",
        "Those are kind of irritating in that they're just numbers. Let's try percentages instead"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoLPm2XDVnmk"
      },
      "source": [
        "#### Logisitic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYww8KpKVnml",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "66319cf3-d924-474a-e3e6-69934e0494f9"
      },
      "source": [
        "y_true = y_test\n",
        "y_pred = logreg.predict(X_test)\n",
        "matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "label_names = pd.Series(['negative', 'positive'])\n",
        "pd.DataFrame(matrix,\n",
        "     columns='Predicted ' + label_names,\n",
        "     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted negative</th>\n",
              "      <th>Predicted positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Is negative</th>\n",
              "      <td>0.736472</td>\n",
              "      <td>0.263528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is positive</th>\n",
              "      <td>0.232913</td>\n",
              "      <td>0.767087</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Predicted negative  Predicted positive\n",
              "Is negative            0.736472            0.263528\n",
              "Is positive            0.232913            0.767087"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69aCQYNDVnml"
      },
      "source": [
        "#### Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4KULHahVnml",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "6c097395-92cb-454f-bbb5-e9aff78ba2d2"
      },
      "source": [
        "y_true = y_test\n",
        "y_pred = logreg.predict(X_test)\n",
        "matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "label_names = pd.Series(['negative', 'positive'])\n",
        "pd.DataFrame(matrix,\n",
        "     columns='Predicted ' + label_names,\n",
        "     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted negative</th>\n",
              "      <th>Predicted positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Is negative</th>\n",
              "      <td>0.736472</td>\n",
              "      <td>0.263528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is positive</th>\n",
              "      <td>0.232913</td>\n",
              "      <td>0.767087</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Predicted negative  Predicted positive\n",
              "Is negative            0.736472            0.263528\n",
              "Is positive            0.232913            0.767087"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHIHMhoWVnmm"
      },
      "source": [
        "#### Random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpvgayDIVnmm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "b886681d-d640-44d2-d12f-b1103ae1b78e"
      },
      "source": [
        "y_true = y_test\n",
        "y_pred = forest.predict(X_test)\n",
        "matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "label_names = pd.Series(['negative', 'positive'])\n",
        "pd.DataFrame(matrix,\n",
        "     columns='Predicted ' + label_names,\n",
        "     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted negative</th>\n",
              "      <th>Predicted positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Is negative</th>\n",
              "      <td>0.747835</td>\n",
              "      <td>0.252165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is positive</th>\n",
              "      <td>0.273396</td>\n",
              "      <td>0.726604</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Predicted negative  Predicted positive\n",
              "Is negative            0.747835            0.252165\n",
              "Is positive            0.273396            0.726604"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yslCTuqlVnmm"
      },
      "source": [
        "#### SVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVNG97X4Vnmm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "1e667552-4139-4cb4-e490-94a58c165179"
      },
      "source": [
        "y_true = y_test\n",
        "y_pred = svc.predict(X_test)\n",
        "matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "label_names = pd.Series(['negative', 'positive'])\n",
        "pd.DataFrame(matrix,\n",
        "     columns='Predicted ' + label_names,\n",
        "     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted negative</th>\n",
              "      <th>Predicted positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Is negative</th>\n",
              "      <td>0.735660</td>\n",
              "      <td>0.264340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is positive</th>\n",
              "      <td>0.231861</td>\n",
              "      <td>0.768139</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Predicted negative  Predicted positive\n",
              "Is negative            0.735660            0.264340\n",
              "Is positive            0.231861            0.768139"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Arq4He4PVnmn"
      },
      "source": [
        "#### Multinomial Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5Uw9usGVnmn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "6d3cd661-8dec-440c-bfab-af5b7eee7320"
      },
      "source": [
        "y_true = y_test\n",
        "y_pred = bayes.predict(X_test)\n",
        "matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "label_names = pd.Series(['negative', 'positive'])\n",
        "pd.DataFrame(matrix,\n",
        "     columns='Predicted ' + label_names,\n",
        "     index='Is ' + label_names).div(matrix.sum(axis=1), axis=0)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted negative</th>\n",
              "      <th>Predicted positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Is negative</th>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is positive</th>\n",
              "      <td>0.258412</td>\n",
              "      <td>0.741588</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Predicted negative  Predicted positive\n",
              "Is negative            0.750000            0.250000\n",
              "Is positive            0.258412            0.741588"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49YDuRP8Vnmn"
      },
      "source": [
        "## Review\n",
        "\n",
        "If you find yourself unsatisfied with a tool, you can try to build your own! This is exactly what we tried to do, using the **Sentiment140 dataset** and several machine learning algorithms.\n",
        "\n",
        "Sentiment140 is a database of tweets that come pre-labeled with positive or negative sentiment, assigned automatically by presence of a `:)` or `:(`.  Our first step was using a **vectorizer** to convert the tweets into numbers a computer could understand.\n",
        "\n",
        "After that, we build four different **models** using different machine learning algorithms. Each one was fed a list of each tweet's **features** - the words - and each tweet's **label** - the sentiment - in the hopes that later it could predict labels if given a new tweets. This process of teaching the algorithm is called **training**.\n",
        "\n",
        "In order to test our algorithms, we split our data into sections - **train** and **test** datasts. You teach the algorithm with the first group, and then ask it for predictions on the second set. You can then compare its predictions to the right answers using a **confusion matrix**.\n",
        "\n",
        "Although **different algorithms took different amounts of time to train**, they all ended up with about 70-75% accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtmAD-1DVnmn"
      },
      "source": [
        "## Discussion topics\n",
        "\n",
        "* Which models performed the best? Were there big differences?\n",
        "* Do you think it's more important to be sensitive to negativity or positivity? Do we want more positive things incorrectly marked as negative, or more negative things marked as positive?\n",
        "* They all had very different training times. Which ones offer the best combination of performance and not making you wait around for an hour?\n",
        "* If you have a decent algorithm that trains more quickly, that could that mean about feature selection or the size of your training set? Why did we use `max_features=` and `df.sample`?\n",
        "* Is 75% accuracy good?\n",
        "* Do your feelings change if the performance is described as \"incorrect one out of every four times?\"\n",
        "* What would your accuracy be for a random guess?\n",
        "* How do you feel about sentiment analysis?\n",
        "* What would you feel comfortable using our sentiment classifier for?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPp92Z2iXm27"
      },
      "source": [
        "# Sentiment Analysis using Libraries\r\n",
        "\r\n",
        "Source: https://colab.research.google.com/github/littlecolumns/ds4j-notebooks/blob/master/sentiment-analysis-is-bad/notebooks/Comparing%20sentiment%20analysis%20tools.ipynb#scrollTo=JVOidBrYXS3r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSd7iJ0DXS31"
      },
      "source": [
        "# NLTK: Natural Language Tooklit\n",
        "\n",
        "[Natural Language Toolkit](https://www.nltk.org/) is the basis for a lot of text analysis done in Python. It's old and terrible and slow, but it's just been used for so long and does so many things that it's generally the default when people get into text analysis. The new kid on the block is [spaCy](https://spacy.io/) (but it doesn't do sentiment analysis out of the box so we're leaving it out of this).\n",
        "\n",
        "When you first run NLTK, you need to download some datasets to make sure it will be able to do everything you want."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TxL3eb_XS32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c68e55fb-73be-45c7-8e7f-09ee94626bbf"
      },
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPbRFMUMXS33"
      },
      "source": [
        "To do sentiment analysis with NLTK, it only takes a couple lines of code. To determine sentiment, it's using a tool called **VADER**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IhWoPRIXS33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae4fe6a9-2dd1-4a72-edc6-f3540ae5a788"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
        "\n",
        "sia = SIA()\n",
        "sia.polarity_scores(\"This restaurant was great, but I'm not sure if I'll go there again.\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'compound': 0.0276, 'neg': 0.153, 'neu': 0.688, 'pos': 0.159}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hap7Nd4XS34"
      },
      "source": [
        "Asking `SentimentIntensityAnalyzer` for the `polarity_score` gave us four values in a dictionary:\n",
        "\n",
        "- **negative:** the negative sentiment in a sentence\n",
        "- **neutral:** the neutral sentiment in a sentence\n",
        "- **positive:** the postivie sentiment in the sentence\n",
        "- **compound:** the aggregated sentiment. \n",
        "    \n",
        "Seems simple enough!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1svKNs6XS34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f880bee1-a69c-4178-d4b5-f9eeeb2bedfa"
      },
      "source": [
        "text = \"I just got a call from my boss - does he realise it's Saturday?\"\n",
        "sia.polarity_scores(text)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'compound': 0.0, 'neg': 0.0, 'neu': 1.0, 'pos': 0.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InUmsLuhXS34"
      },
      "source": [
        "Just like in real life, if you use an emoticon you can be read as being more positive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PkncLuaXS35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "002feb9a-1a7c-42d9-963f-164455943478"
      },
      "source": [
        "text = \"I just got a call from my boss - does he realise it's Saturday? :)\"\n",
        "sia.polarity_scores(text)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'compound': 0.4588, 'neg': 0.0, 'neu': 0.786, 'pos': 0.214}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKH4IYQ7XS35"
      },
      "source": [
        "But what if we swap out the emoticon for an emoji?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zIrifEsXS35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd95b28d-3171-4642-a4a9-3ee5e9ca29bf"
      },
      "source": [
        "text = \"I just got a call from my boss - does he realise it's Saturday? 😊\"\n",
        "sia.polarity_scores(text)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'compound': 0.0, 'neg': 0.0, 'neu': 1.0, 'pos': 0.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOyXJHOQXS36"
      },
      "source": [
        "Back to neutral! Why didn't it understand the emoji the same way it understood the emoticon? Well, **text analysis tools only knows the words that they've been taught,** and if VADER's never seen 😊 before it won't know what to think of it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObpjKZbwXS36"
      },
      "source": [
        "# TextBlob\n",
        "\n",
        "TextBlob is built on top of NLTK, but is infinitely easier to use. It's still slow, but _it's so so so easy to use_. \n",
        "\n",
        "You can just feed TextBlob your sentence, then ask for a `.sentiment`!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jYeGUjrXS36"
      },
      "source": [
        "from textblob import TextBlob\n",
        "from textblob import Blobber\n",
        "from textblob.sentiments import NaiveBayesAnalyzer"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LMcjlayXS36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89ee38bb-ff08-4d16-b075-6a80b81fe52d"
      },
      "source": [
        "blob = TextBlob(\"This restaurant was great, but I'm not sure if I'll go there again.\")\n",
        "blob.sentiment"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=0.275, subjectivity=0.8194444444444444)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrh7s1jbXS37"
      },
      "source": [
        "**How could it possibly be easier than that?!?!?** This time we get a `polarity` and a `subjectivity` instead of all of those different scores, but it's basically the same idea.\n",
        "\n",
        "If you like options: it turns out TextBlob actually has multiple sentiment analysis tools! How fun! We can plug in a different analyzer to get a different result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQs6jc5wXS37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b65a7e0d-dad9-4f3e-c0ae-f0b20b45bc66"
      },
      "source": [
        "blobber = Blobber(analyzer=NaiveBayesAnalyzer())\n",
        "\n",
        "blob = blobber(\"This restaurant was great, but I'm not sure if I'll go there again.\")\n",
        "blob.sentiment"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(classification='pos', p_pos=0.5879425317005774, p_neg=0.41205746829942275)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hHxYPuhXS37"
      },
      "source": [
        "Wow, that's a **very different result.** To understand why it's so different, we need to talk about where these sentiment numbers come from."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bBtYFXDXS38"
      },
      "source": [
        "# But where do those numbers come from?\n",
        "\n",
        "The most important thing to understand is **sentiment is always just an opinion.** In this case it's an opinion, yes, but specifically **the opinion of a machine.**\n",
        "\n",
        "## VADER\n",
        "\n",
        "NLTK's Sentiment Intensity Analyzer works is using something called **VADER**, which is a list of words that have a sentiment associated with each of them.\n",
        "\n",
        "|Word|Sentiment rating|\n",
        "|---|---|\n",
        "|tragedy|-3.4|\n",
        "|rejoiced|2.0|\n",
        "|disaster|-3.1|\n",
        "|great|3.1|\n",
        "\n",
        "If you have more positives, the sentence is more positive. If you have more negatives, it's more negative. It can also take into account things like capitalization - you can read more about the classifier [here](http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html), or the actual paper it came out of [here](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf).\n",
        "\n",
        "**How do they know what's positive/negative?** They came up with a very big list of words, then asked people on the internet and paid them one cent for each word they scored.\n",
        "\n",
        "## TextBlob's `.sentiment`\n",
        "\n",
        "TextBlob's sentiment analysis is based on a separate library called [pattern](https://www.clips.uantwerpen.be/pattern).\n",
        "\n",
        "> The sentiment analysis lexicon bundled in Pattern focuses on adjectives. It contains adjectives that occur frequently in customer reviews, hand-tagged with values for polarity and subjectivity.\n",
        "\n",
        "Same kind of thing as NLTK's VADER, but it specifically looks at words from customer reviews.\n",
        "\n",
        "**How do they know what's positive/negative?** They look at (mostly) adjectives that occur in customer reviews and hand-tag them.\n",
        "\n",
        "## TextBlob's `.sentiment` + NaiveBayesAnalyzer\n",
        "\n",
        "TextBlob's other option uses a `NaiveBayesAnalyzer`, which is a machine learning technique. When you use this option with TextBlob, the sentiment is coming from \"an NLTK classifier trained on a movie reviews corpus.\"\n",
        "\n",
        "**How do they know what's positive/negative?** Looked at movie reviews and scores using machine learning, the computer _automatically learned_ what words are associated with a positive or negative rating.\n",
        "\n",
        "## What's this mean for me?\n",
        "\n",
        "When you're doing sentiment analysis with tools like this, you should have a few major questions: \n",
        "\n",
        "* Where kind of dataset does the list of known words come from?\n",
        "* Do they use all the words, or a selection of the words?\n",
        "* Where do the positive/negative scores come from?\n",
        "\n",
        "Let's compare the tools we've used so far.\n",
        "\n",
        "|technique|word source|word selection|scores|\n",
        "|---|---|---|---|\n",
        "|NLTK (VADER)|everywhere|hand-picked|internet people, word-by-word|\n",
        "|TextBlob|product reviews|hand-picked, mostly adjectives|internet people, word-by-word|\n",
        "|TextBlob + NaiveBayesAnalyzer|movie reviews|all words|automatic based on score|\n",
        "\n",
        "A major thing that should jump out at you is **how different the sources are.**\n",
        "\n",
        "While VADER focuses on content found everywhere, TextBlob's two options are specific to certain domains. The [original paper for VADER](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf) passive-aggressively noted that VADER is effective at general use, but being trained on a specific domain can have benefits: \n",
        "\n",
        "> While some algorithms performed decently on test data from the specific domain for which it was expressly trained, they do not significantly outstrip the simple model we use.\n",
        "\n",
        "They're basically saying, \"if you train a model on words from a certain field, it will be good at sentiment in that certain field.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy5lXak8XS38"
      },
      "source": [
        "# Analyzing differences in sentiment analysis tools\n",
        "\n",
        "Because they're build differently, sentiment analysis tools don't always agree. Let's take a set of sentences and compare each analyzer's understanding of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvCi0xCGXS39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "9762c43e-2ad4-4160-feed-69fb60b80a9a"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "\n",
        "df = pd.DataFrame({'content': [\n",
        "    \"I love love love love this kitten\",\n",
        "    \"I hate hate hate hate this keyboard\",\n",
        "    \"I'm not sure how I feel about toast\",\n",
        "    \"Did you see the baseball game yesterday?\",\n",
        "    \"The package was delivered late and the contents were broken\",\n",
        "    \"Trashy television shows are some of my favorites\",\n",
        "    \"I'm seeing a Kubrick film tomorrow, I hear not so great things about it.\",\n",
        "    \"I find chirping birds irritating, but I know I'm not the only one\",\n",
        "]})\n",
        "df"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I love love love love this kitten</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I hate hate hate hate this keyboard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I'm not sure how I feel about toast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Did you see the baseball game yesterday?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The package was delivered late and the contents were broken</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Trashy television shows are some of my favorites</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I'm seeing a Kubrick film tomorrow, I hear not so great things about it.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I find chirping birds irritating, but I know I'm not the only one</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                    content\n",
              "0                                         I love love love love this kitten\n",
              "1                                       I hate hate hate hate this keyboard\n",
              "2                                       I'm not sure how I feel about toast\n",
              "3                                  Did you see the baseball game yesterday?\n",
              "4               The package was delivered late and the contents were broken\n",
              "5                          Trashy television shows are some of my favorites\n",
              "6  I'm seeing a Kubrick film tomorrow, I hear not so great things about it.\n",
              "7         I find chirping birds irritating, but I know I'm not the only one"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXTbNM4lXS39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "a6e6af1a-0ddc-414f-f279-f63d82e62b94"
      },
      "source": [
        "def get_scores(content):\n",
        "    blob = TextBlob(content)\n",
        "    nb_blob = blobber(content)\n",
        "    sia_scores = sia.polarity_scores(content)\n",
        "    \n",
        "    return pd.Series({\n",
        "        'content': content,\n",
        "        'textblob': blob.sentiment.polarity,\n",
        "        'textblob_bayes': nb_blob.sentiment.p_pos - nb_blob.sentiment.p_neg,\n",
        "        'nltk': sia_scores['compound'],\n",
        "    })\n",
        "\n",
        "scores = df.content.apply(get_scores)\n",
        "scores.style.background_gradient(cmap='RdYlGn', axis=None, low=0.4, high=0.4)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row0_col1{\n",
              "            background-color:  #c3e67d;\n",
              "            color:  #000000;\n",
              "        }#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row0_col2{\n",
              "            background-color:  #fff6b0;\n",
              "            color:  #000000;\n",
              "        }#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row0_col3{\n",
              "            background-color:  #73c264;\n",
              "            color:  #000000;\n",
              "        }#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row1_col1{\n",
              "            background-color:  #fa9656;\n",
              "            color:  #000000;\n",
              "        }#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row1_col2{\n",
              "            background-color:  #feeb9d;\n",
              "            color:  #000000;\n",
              "        }#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row1_col3{\n",
              "            background-color:  #f67a49;\n",
              "            color:  #000000;\n",
              "        }#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row2_col1,#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row7_col3{\n",
              "            background-color:  #fee797;\n",
              "            color:  #000000;\n",
              "        }#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row2_col2{\n",
              "            background-color:  #d3ec87;\n",
              "            color:  #000000;\n",
              "        }#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row2_col3{\n",
              "            background-color:  #fee999;\n",
              "            color:  #000000;\n",
              "        }#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row3_col1{\n",
              "            background-color:  #fed683;\n",
              "            color:  #000000;\n",
              "        }#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row3_col2{\n",
              "            background-color:  #b1de71;\n",
              "            color:  #000000;\n",
              "        }#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row3_col3,#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row5_col1{\n",
              "            background-color:  #fffebe;\n",
              "            color:  #000000;\n",
              "        }#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row4_col1{\n",
              "            background-color:  #fede89;\n",
              "            color:  #000000;\n",
              "        }#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row4_col2{\n",
              "            background-color:  #fdbd6d;\n",
              "            color:  #000000;\n",
              "        }#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row4_col3{\n",
              "            background-color:  #feca79;\n",
              "            color:  #000000;\n",
              "        }#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row5_col2{\n",
              "            background-color:  #fbfdba;\n",
              "            color:  #000000;\n",
              "        }#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row5_col3{\n",
              "            background-color:  #cfeb85;\n",
              "            color:  #000000;\n",
              "        }#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row6_col1{\n",
              "            background-color:  #91d068;\n",
              "            color:  #000000;\n",
              "        }#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row6_col2{\n",
              "            background-color:  #a0d669;\n",
              "            color:  #000000;\n",
              "        }#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row6_col3{\n",
              "            background-color:  #fdb567;\n",
              "            color:  #000000;\n",
              "        }#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row7_col1{\n",
              "            background-color:  #feec9f;\n",
              "            color:  #000000;\n",
              "        }#T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row7_col2{\n",
              "            background-color:  #e3f399;\n",
              "            color:  #000000;\n",
              "        }</style><table id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >content</th>        <th class=\"col_heading level0 col1\" >textblob</th>        <th class=\"col_heading level0 col2\" >textblob_bayes</th>        <th class=\"col_heading level0 col3\" >nltk</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row0_col0\" class=\"data row0 col0\" >I love love love love this kitten</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row0_col1\" class=\"data row0 col1\" >0.500000</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row0_col2\" class=\"data row0 col2\" >-0.087933</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row0_col3\" class=\"data row0 col3\" >0.957100</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row1_col0\" class=\"data row1 col0\" >I hate hate hate hate this keyboard</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row1_col1\" class=\"data row1 col1\" >-0.800000</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row1_col2\" class=\"data row1 col2\" >-0.214151</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row1_col3\" class=\"data row1 col3\" >-0.941300</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row2_col0\" class=\"data row2 col0\" >I'm not sure how I feel about toast</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row2_col1\" class=\"data row2 col1\" >-0.250000</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row2_col2\" class=\"data row2 col2\" >0.394659</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row2_col3\" class=\"data row2 col3\" >-0.241100</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row3_col0\" class=\"data row3 col0\" >Did you see the baseball game yesterday?</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row3_col1\" class=\"data row3 col1\" >-0.400000</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row3_col2\" class=\"data row3 col2\" >0.613050</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row3_col3\" class=\"data row3 col3\" >0.000000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row4_col0\" class=\"data row4 col0\" >The package was delivered late and the contents were broken</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row4_col1\" class=\"data row4 col1\" >-0.350000</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row4_col2\" class=\"data row4 col2\" >-0.574270</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row4_col3\" class=\"data row4 col3\" >-0.476700</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row5_col0\" class=\"data row5 col0\" >Trashy television shows are some of my favorites</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row5_col1\" class=\"data row5 col1\" >0.000000</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row5_col2\" class=\"data row5 col2\" >0.040076</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row5_col3\" class=\"data row5 col3\" >0.421500</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row6_col0\" class=\"data row6 col0\" >I'm seeing a Kubrick film tomorrow, I hear not so great things about it.</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row6_col1\" class=\"data row6 col1\" >0.800000</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row6_col2\" class=\"data row6 col2\" >0.717875</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row6_col3\" class=\"data row6 col3\" >-0.629600</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row7_col0\" class=\"data row7 col0\" >I find chirping birds irritating, but I know I'm not the only one</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row7_col1\" class=\"data row7 col1\" >-0.200000</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row7_col2\" class=\"data row7 col2\" >0.257148</td>\n",
              "                        <td id=\"T_01f72a70_7bb3_11eb_87e2_0242ac1c0002row7_col3\" class=\"data row7 col3\" >-0.250000</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7faf82e21b10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaNOyLRHXS3-"
      },
      "source": [
        "Wow, those really don't agree with one another! Which one do you agree with the most? Did it get everything \"right?\"\n",
        "\n",
        "While it seemed like magic to be able to plug a sentence into a sentiment analyzer and get a result back... maybe things aren't as magical as we thought."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tja2BugmXS3-"
      },
      "source": [
        "# Review\n",
        "\n",
        "**Sentiment analysis** is judging whether a piece of text has positive or negative emotion. We covered several tools for doing automatic sentiment analysis: **NLTK**, and two techniques inside of **TextBlob**.\n",
        "\n",
        "Each tool uses a different data to determine what is positive and negative, and while some use **humans** to flag things as positive or negative, others use a automatic **machine learning**.\n",
        "\n",
        "As a result of these differences, each tool can come up with very **different sentiment scores** for the same piece of text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDN1cbeaXS3_"
      },
      "source": [
        "# Discussion topics\n",
        "\n",
        "The first questions are about whether an analyzer can be applied in situations other than where it was trained. Among other things, you'll want to think about whether the language it was trained on is similar to the language you're using it on.\n",
        "\n",
        "**Is it okay to use a sentiment analyzer built on product reviews to check the sentiment of tweets?** How about to check the sentiment of wine reviews?\n",
        "\n",
        "**Is it okay to use a sentiment analyzer trained on everything to check the sentiment of tweets?** How about to check the sentiment of wine reviews?\n",
        "\n",
        "**Let's say it's a night of political debates.** If I'm trying to report on whether people generally like or dislike what is happening throughout the debates, could I use these sorts of tools on tweets?\n",
        "\n",
        "We're using the incredibly vague word \"okay\" on purpose, as there are varying levels of comfort depending on your sitaution. Are you doing this for preliminary research? Are you publishing the results in a journal, in a newspaper, in a report at work, in a public policy recommendation? What if I tell you that the ideal of \"I'd only use a sentiment analysis tool trained exactly for my specific domain\" is both _rare and impractical?_\n",
        "\n",
        "As we saw in the last section, **these tools don't always agree with one another, which might be problematic.**\n",
        "\n",
        "* What might make them agree or disagree?\n",
        "* Do we think one is the \"best?\"\n",
        "* Can you think of any ways to test which one is the 'best' for our purposes?"
      ]
    }
  ]
}